- name: ' Konductor | Provision UPI Infra | configure-iac-serviceaccount-token-and-kubeconfig.yml'
  hosts: localhost
  vars_files:
    - 'vars/vault.yml'
    - 'vars/global.yml'
    - 'vars/infra.yml'
  vars:
    ansible_python_interpreter: /usr/bin/python3
    module: "Create IAC service account and kubeconfig "
    ansible_name_module: " Post Cluster Installation | {{ module }}"

  pre_tasks:
    - name: '{{ ansible_name_module }} | validate AWS S3 Bucket creds if applicable'
      when:
        - copy_to_s3 is defined
        - copy_to_s3 | bool
      block:
        - name: '{{ ansible_name_module }} | assert | The AWS access key is defined '
          ansible.builtin.assert:
            that:
              - aws_access_key is defined
              - aws_access_key != ''
            msg: "The AWS cloud credentials access key is required to run this playbook."
          when:
            - not aws_access_key is defined or aws_access_key == ''

        - name: '{{ ansible_name_module }} | assert | The AWS access secret is defined '
          ansible.builtin.assert:
            that:
              - aws_access_secret is defined
              - aws_access_secret != ''
            msg: "The AWS cloud credentials access secret is required to run this playbook."
          when:
            - not aws_access_secret is defined or aws_access_secret == ''

    - name: Ensure Proper Python dependency is installed for Openshift
      community.general.python_requirements_info:
        dependencies:
          - openshift
          - requests

    - name: Authenticate with the API
      ansible.builtin.command: >
        {{ openshift_cli }} login \
          --token {{ ocp_cluster_token }} \
          --insecure-skip-tls-verify=true {{ ocp_cluster_console_url }}:{{ ocp_cluster_console_port | d('6443', true) }}
      register: login_out

    - name: Authenticate with the API
      ansible.builtin.command: >
        {{ openshift_cli }} login \
          -u {{ ocp_cluster_user }} \
          -p {{ ocp_cluster_user_password }} \
          --insecure-skip-tls-verify=true {{ ocp_cluster_console_url }}:{{ ocp_cluster_console_port | d('6443', true) }}
      register: login_out

  tasks:
    - name: '{{ ansible_name_module }} | Retrieve Infrastructure Name'
      ansible.builtin.command: >
        {{ openshift_cli }} get infrastructure cluster -o jsonpath='{.status.infrastructureName}{"\n"}'
      register: cluster_infra_name

    - name: '{{ ansible_name_module }} | set_fact | cluster_name '
      ansible.builtin.set_fact:
        cluster_name: "{{ cluster_infra_name.stdout }}"
      when:
        - cluster_infra_name.stdout is defined
        - cluster_infra_name.stdout != ''

    - name: '{{ ansible_name_module }} | set_fact | kubeconfig_staging_dir '
      ansible.builtin.set_fact:
        kubeconfig_staging_dir: "{{ staging_dir | d('/tmp', true) }}/{{ cluster_name }}"
      when:
        - not kubeconfig_staging_dir is defined or not kubeconfig_staging_dir != ''

    - name: '{{ ansible_name_module }} | ansible.builtin.file | Ensure the kubeconfig file stagin dir exist '
      ansible.builtin.file:
        path: "{{ kubeconfig_staging_dir }}"
        state: directory
        mode: 0777

    - name: '{{ ansible_name_module }} | Create kubeconfig Namespace'
      when:
        - platform_infra_ns is defined
        - platform_infra_ns != '' 
      block:
        ##### Adding this to bypass restrictions on creating project with names starting with openshift  #####
        - name: '{{ ansible_name_module }} |  Create kubeconfig download Namespace'
          ansible.builtin.command: >
            {{ openshift_cli }} create namespace '{{ platform_infra_ns }}'
          failed_when: "kubeconfig_ns_created.rc >=1 and not ' already exists' in kubeconfig_ns_created.stderr"
          when:
            - "'openshift' in platform_infra_ns"
          register: kubeconfig_ns_created

        - name: '{{ ansible_name_module }} |  Create kubeconfig download Namespace'
          ansible.builtin.command: >
            {{ openshift_cli }} new-project '{{ platform_infra_ns }}' \
              --display-name='{{ platform_infra_ns_description }}' \
              --description='{{ platform_infra_ns_description }}'
          failed_when: "kubeconfig_ns_created.rc >=1 and not ' already exists' in kubeconfig_ns_created.stderr"
          when:
            - "not 'openshift' in platform_infra_ns"
          register: kubeconfig_ns_created

    - name: '{{ ansible_name_module }} | create service account'
      ansible.builtin.command: >
        {{ openshift_cli }} create serviceaccount {{ kubeconfig_service_account }} -n {{ platform_infra_ns | default('kube-system', true) }}
      failed_when: "sa_created.rc > 0 and not ' already exists' in sa_created.stderr"
      register: sa_created

    - name: '{{ ansible_name_module }} | Grant cluster-role to service account'
      ansible.builtin.command: >
        {{ openshift_cli }} adm policy \
          add-cluster-role-to-user cluster-admin -z {{ kubeconfig_service_account }}  \
           -n {{ platform_infra_ns | default('kube-system', true) }}
      failed_when:
        - cluster_role_granted.rc >= 1
        - not 'AlreadyExists' in cluster_role_granted.stderr
      register: cluster_role_granted

    - name: '{{ ansible_name_module }} | Create SA Token'
      ansible.builtin.command: >
        {{ openshift_cli }} create token {{ kubeconfig_service_account }} \
           --duration={{ kubeconfig_sa_token_duration | default("8760h", true) }} \
             -n {{ platform_infra_ns | default('kube-system', true) }}
      register: kubeconfig_sa_token_retrieved

    - name: '{{ ansible_name_module }} | copy | write SA token to file'
      ansible.builtin.copy:
        content: "{{ kubeconfig_sa_token_retrieved.stdout }}" 
        dest: "{{ kubeconfig_staging_dir }}/{{ cluster_name }}-{{ kubeconfig_service_account }}-token.txt" 
        force: yes
      register: kubeconfig_sa_token_written

    - name: '{{ ansible_name_module }} | Retrieve current kubeconfig CA '
      ansible.builtin.command: >
        {{ openshift_cli }} config refresh-ca-bundle --dry-run
      register: kubeconfig_ca_retrieved

    - name: '{{ ansible_name_module }} | ansible.builtin.debug | Print CA output '
      ansible.builtin.debug:
        var: kubeconfig_ca_retrieved.stdout 
        verbosity: 2

    - name: '{{ ansible_name_module }} | write CA to file '
      ansible.builtin.copy:
        content: "{{ kubeconfig_ca_retrieved.stdout }}"
        dest: "{{ kubeconfig_staging_dir }}/{{ cluster_name }}-kubeconfig-ca.crt"
        force: true
      register: kubeconfig_exported

    - name: '{{ ansible_name_module }} | retrieve API endpoint '
      ansible.builtin.command: >
        {{ openshift_cli }} whoami --show-server 
      register: kubeconfig_api_retrieved

    - name: '{{ ansible_name_module }} | ansible.builtin.debug | Print API output '
      ansible.builtin.debug:
        var: kubeconfig_api_retrieved.stdout 
        verbosity: 2

    - name: '{{ ansible_name_module }} | set_fact | kubeconfig_ca_file '
      ansible.builtin.set_fact:
        kubeconfig_ca_file: "{{ kubeconfig_staging_dir }}/{{ cluster_name }}-kubeconfig-ca.crt"

    - name: '{{ ansible_name_module }} | set_fact | kubeconfig_file '
      ansible.builtin.set_fact:
        kubeconfig_file: "{{ kubeconfig_staging_dir }}/{{ cluster_name }}-kubeconfig"

    - name: '{{ ansible_name_module }} | create kubeconfig file for SA '
      ansible.builtin.command: >
        {{ openshift_cli }} login --token={{ kubeconfig_sa_token_retrieved.stdout }} \
         --certificate-authority={{ kubeconfig_ca_file }} \
         --server={{ kubeconfig_api_retrieved.stdout }}
      register: kubeconfig_created

    - name: '{{ ansible_name_module }} | ansible.builtin.debug | Print kubeconfig output '
      ansible.builtin.debug:
        var: kubeconfig_created.stdout 
        verbosity: 2

    - name: '{{ ansible_name_module }} | Retrieve current context'
      ansible.builtin.command: >
        {{ openshift_cli }} config current-context
      register: current_context_retrieved

    - name: '{{ ansible_name_module }} | ansible.builtin.debug | Print Current Context output '
      ansible.builtin.debug:
        var: current_context_retrieved.stdout 
        verbosity: 2

    - name: '{{ ansible_name_module }} | Retrieve current cluster'
      ansible.builtin.command: >
        {{ openshift_cli }} config view -o jsonpath='{.contexts[?(@.name == "{{ current_context_retrieved.stdout }}")].context.cluster}{"\n"}' 
      register: current_cluster_retrieved

    - name: '{{ ansible_name_module }} | ansible.builtin.debug | Print Current Context output '
      ansible.builtin.debug:
        var: current_cluster_retrieved.stdout 
        verbosity: 2

    - name: '{{ ansible_name_module }} | template | Render kubeconfig to file'
      ansible.builtin.template:
        src: "templates/kubeconfig.yaml.j2" 
        dest: "{{ kubeconfig_file }}" 
        force: yes
      vars:
        cluster_ca_data: "{{ lookup('file', kubeconfig_ca_file) | b64encode }}"
        cluter_api: "{{ kubeconfig_api_retrieved.stdout }}"
        current_cluster: "{{ current_cluster_retrieved.stdout }}"
        sa_user_current_context: "{{ current_context_retrieved.stdout }}"
        sa_user_token: "{{ kubeconfig_sa_token_retrieved.stdout }}"
      register: kubeconfig_exported 

    - name: '{{ ansible_name_module }} | write kubeconfig to file '
      ansible.builtin.shell: >
        cat {{ kubeconfig_file }}
      register: kubeconfig_printed

    - name: '{{ ansible_name_module }} | ansible.builtin.debug | Print kubeconfig file output '
      ansible.builtin.debug:
        var: kubeconfig_printed.stdout 
        verbosity: 2

    - name: '{{ ansible_name_module }} | Copy IAC token to S3 Bucket if applicable'
      when:
        - copy_to_s3 is defined
        - copy_to_s3 | bool 
      block:
        - name: '{{ ansible_name_module }} | command:which | Check if aws cli is installed'
          ansible.builtin.shell: >
            which aws
          ignore_errors: yes
          register: aws_binary

        - name: '{{ ansible_name_module }} | assert | the AWS CLI binary is defined'
          ansible.builtin.assert:
            that:
              - aws_binary is defined
              - aws_binary.rc is defined
              - aws_binary.rc == 0
              - aws_binary.stdout is defined
              - aws_binary.stdout != ''
            msg: "The AWS CLI binary is required to upload the raw manifest to the S3 bucket "

        - name: '{{ ansible_name_module }} | set_fact | aws_cli '
          ansible.builtin.set_fact:
            aws_cli: '{{ aws_binary.stdout }}'

        - name: '{{ ansible_name_module }} | Set Bucket name'
          ansible.builtin.set_fact:
            kubeconfig_aws_bucket: "{{ bucket_name_prefix}}{{ cluster_name }}-state-backup"
          when:
            - not kubeconfig_aws_bucket is defined or kubeconfig_aws_bucket == ''

        - name: '{{ ansible_name_module }} | Retrieve Infrastructure Region'
          ansible.builtin.command: >
            {{ openshift_cli }} get -o jsonpath='{.status.platformStatus.aws.region}{"\n"}' infrastructure cluster
          when:
            - not aws_region is defined or aws_region == ''
          register: cluster_aws_region

        - name: '{{ ansible_name_module }} | Set Bucket region'
          ansible.builtin.set_fact:
            aws_region: "{{ cluster_aws_region.stdout }}"
          when:
            - cluster_aws_region is defined
            - cluster_aws_region.rc is defined
            - cluster_aws_region.rc == 0
            - cluster_aws_region.stdout is defined
            - cluster_aws_region.stdout != ''

        - name: '{{ ansible_name_module }} | Set Bucket endpoint url string'
          ansible.builtin.set_fact:
            s3_endpoint_srg: "{{ ('--endpoint-url ' + s3url ) if use_s3url is defined and use_s3url | bool and s3url is defined and s3url != '' else '' }}" 

        - name: '{{ ansible_name_module }} | command:shell | create S3 bucket if it does not already exist'
          ansible.builtin.shell: |
            {{ aws_cli }} s3 ls s3://{{ kubeconfig_aws_bucket }} {{ s3_endpoint_srg }} --region {{ aws_region }} 2>&1 ||  {{ aws_cli }} s3 mb s3://{{ kubeconfig_aws_bucket }} {{ s3_endpoint_srg }} --region {{ aws_region }} 
          environment:
            AWS_ACCESS_KEY_ID: "{{ aws_access_key }}"
            AWS_SECRET_ACCESS_KEY: "{{ aws_access_secret }}"
          register: cluster_aws_region

        - name: '{{ ansible_name_module }} | command:shell | copy IAC token files directory to S3 bucket'
          ansible.builtin.shell: >
            {{ aws_cli }} s3 --region {{ aws_region }} sync {{ kubeconfig_staging_dir }}/ \
             s3://{{ kubeconfig_aws_bucket }}/iac-token-files {{ s3_endpoint_srg }}  
          environment:
            AWS_ACCESS_KEY_ID: "{{ aws_access_key }}"
            AWS_SECRET_ACCESS_KEY: "{{ aws_access_secret }}"
          register: iac_files_uploaded

